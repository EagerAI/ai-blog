---
title: "Simple Audio Classification with Torch"
description: |
  A short description of the post.
Athos Damiani
date: 01-22-2021
output:
  distill::distill_article:
    self_contained: false
---


```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  eval = identical(Sys.getenv("TORCH_TEST", unset = "0"), "1")
)
```

This article attempts to translate the [Daniel Falbel's](https://github.com/dfalbel) ['Simple Audio Classification'](https://blogs.rstudio.com/ai/posts/2018-06-06-simple-audio-classification-keras/) article from `{tensorflow}+{keras}` to `{torch}+{torchaudio}`. The main goal of this article is to introduce {torchaudio}, and to summarise its role, it contributes with the dataset, the audio loader and the spectrogram transformer. An interesting side product is the parallel between torch and tensorflow, showing sometimes the differences, sometimes the similarities between them.

<!-- https://towardsdatascience.com/a-tale-of-two-frameworks-985fa7fcec#6f89 -->

```{r setup}
library(torchaudio)
library(torch)
```

## Downloading and Importing

torchaudio has `speechcommand_dataset` built in. It filter out `_backgroud_noise_` by default and let us choose between versions `v0.01` and `v0.02`.

```{r}
# set a existing folder here to cache the dataset
DATASETS_PATH <- "/media/athos/DATA/OneDrive/Documents/datasets/"

# 1.4GB download
df <- speechcommand_dataset(
  root = DATASETS_PATH, 
  url = "speech_commands_v0.01",
  download = TRUE
)

# expect folder: _backgroud_noise_
df$EXCEPT_FOLDER

# number of audio files
length(df)

# a sample
sample <- df[1]

sample$waveform[, 1:10]
sample$sample_rate
sample$label

plot(sample$waveform[1], type = "l", col = "royalblue", main = sample$label)
```

## Device

```{r}
device <- torch::torch_device(if(torch::cuda_is_available()) "cuda" else "cpu")
```

## Classes

```{r}
df$classes
```


## ~~Generator~~ Dataloader

`torch::dataloader` has the same task as `data_generator` defined in the original article. It is responsible for preparing batches - including shuffling, padding, one-hot encoding, etc - and for taking care of paralellism/device I/O orchestration.

In torch we do this by passing the train/test subset to `torch::dataloader` and all the batch setup logic inside a `collate_fn()` function.

```{r}
set.seed(6)
id_train <- sample(length(df), size = 0.7*length(df))
id_test <- setdiff(seq_len(length(df)), id_train)
# subsets

train_subset <- torch::dataset_subset(df, id_train)
test_subset <- torch::dataset_subset(df, id_test)
```

At this point, `dataloader(train_subset)` would not work because the samples are not padded. So we need build our own `collate_fn()` with the padding strategy.

I suggest using the following approach for help implementing the `collate_fn()`:

1) begin with `collate_fn <- function(batch) browser()`.
2) instantiate `dataloader` with the `collate_fn()`
3) create an environment by calling `enumerate(dataloader)` so you can ask to retrieve a batch from dataloader.
4) run `environment[[1]][[1]]`. Now you should be sent inside collate_fn() with access to `batch` input object. 
5) Build the logic.

```{r, eval = FALSE}
collate_fn <- function(batch) {
  browser()
}

ds_train <- dataloader(
  train_subset, 
  batch_size = 32, 
  shuffle = TRUE, 
  collate_fn = collate_fn
)

ds_train_env <- enumerate(ds_train)
ds_train_env[[1]][[1]]
```

The final `collate_fn()` pad waveform to length 16001 and then stack them up together. At this point there are no spectrograms yet. We going to put spectrogram transformation as part of model architecture.

```{r}
pad_sequence <- function(batch) {
    # Make all tensor in a batch the same length by padding with zeros
    batch <- sapply(batch, function(x) (x$t()))
    batch <- torch::nn_utils_rnn_pad_sequence(batch, batch_first = TRUE, padding_value = 0.)
    return(batch$permute(c(1, 3, 2)))
  }

# Final collate_fn
collate_fn <- function(batch) {
 # Input structure:
 # list of 32 lists: list(waveform, sample_rate, label, speaker_id, utterance_number)
  
 # Transpose it
 batch <- purrr::transpose(batch)
 tensors <- batch$waveform
 targets <- batch$label_index

 # Group the list of tensors into a batched tensor
 tensors <- pad_sequence(tensors)
 
 # target encoding
 targets <- torch::torch_stack(targets)

 list(tensors = tensors, targets = targets) # (64, 1, 16001)
}
```

Batch structure is:

- batch[[1]]: **waveforms** - `tensor` with dimension (32, 1, 16001)
- batch[[2]]: **targets** - `tensor` with dimension (32, 1)

```{r}
ds_train <- dataloader(
  train_subset, 
  batch_size = 64, 
  shuffle = TRUE, 
  drop_last = TRUE,
  collate_fn = collate_fn,
  num_workers = 16,
  worker_globals = c("pad_sequence") # pad_sequence is needed for collect_fn
)

ds_test <- dataloader(
  test_subset, 
  batch_size = 64, 
  shuffle = FALSE, 
  collate_fn = collate_fn,
  num_workers = 12,
  worker_globals = c("pad_sequence") # pad_sequence is needed for collect_fn
)
```

## Model definition

Instead of `keras::keras_model_sequential()` we going to define an `torch::nn_module()`. As referenced by the original article, the model is based on [this architecture for MNIST from this tutorial](https://keras.rstudio.com/articles/examples/mnist_cnn.html) and I'll call it 'DanielNN'.

```{r}
dan_nn <- torch::nn_module(
  "DanielNN",
  
  initialize = function(
    window_size_ms = 30, 
    window_stride_ms = 10
  ) {
    
    # spectrogram spec
    window_size <- as.integer(16000*window_size_ms/1000)
    stride <- as.integer(16000*window_stride_ms/1000)
    fft_size <- as.integer(2^trunc(log(window_size, 2) + 1))
    n_chunks <- length(seq(0, 16000, stride))
    
    self$spectrogram <- torchaudio::transform_spectrogram(
      n_fft = fft_size, 
      win_length = window_size, 
      hop_length = stride, 
      normalized = TRUE, 
      power = 2
    )
    
    # convs 2D
    self$conv1 <- torch::nn_conv2d(in_channels = 1, out_channels = 32, kernel_size = c(3,3))
    self$conv2 <- torch::nn_conv2d(in_channels = 32, out_channels = 64, kernel_size = c(3,3))
    self$conv3 <- torch::nn_conv2d(in_channels = 64, out_channels = 128, kernel_size = c(3,3))
    self$conv4 <- torch::nn_conv2d(in_channels = 128, out_channels = 256, kernel_size = c(3,3))
    
    # denses
    self$dense1 <- torch::nn_linear(in_features = 14336, out_features = 128)
    self$dense2 <- torch::nn_linear(in_features = 128, out_features = 30)
  },
  
  forward = function(x) {
    x %>% # (64, 1, 16001)
      self$spectrogram() %>% # (64, 1, 257, 101)
      torch::torch_abs() %>% 
      torch::torch_add(0.01) %>%
      torch::torch_log() %>%
      self$conv1() %>%
      torch::nnf_relu() %>%
      torch::nnf_max_pool2d(kernel_size = c(2,2)) %>%
      
      self$conv2() %>%
      torch::nnf_relu() %>%
      torch::nnf_max_pool2d(kernel_size = c(2,2)) %>%
      
      self$conv3() %>%
      torch::nnf_relu() %>%
      torch::nnf_max_pool2d(kernel_size = c(2,2)) %>%
      
      self$conv4() %>%
      torch::nnf_relu() %>%
      torch::nnf_max_pool2d(kernel_size = c(2,2)) %>%
      
      torch::nnf_dropout(p = 0.25) %>%
      torch::torch_flatten(start_dim = 2) %>%
      
      self$dense1() %>%
      torch::nnf_relu() %>%
      torch::nnf_dropout(p = 0.5) %>%
      self$dense2() 
  }
)

model <- dan_nn()
model$to(device = device)

print(model)
```

## Model fitting

Unlike tensorflow, there is no `model %>% compile(...)` step in torch, so are going to set `loss criterion`, `optimizer strategy` and `evaluation metrics` explicitly in the training loop.

```{r}
loss_criterion <- torch::nn_cross_entropy_loss()
optimizer <- torch::optim_adadelta(model$parameters)
metrics <- list(acc = yardstick::accuracy_vec)
```

### Training loop

```{r}
library(glue)
library(progress)

pred_to_r <- function(x) {
  factor(as.numeric(x$to(device = "cpu")), levels = 1:30)
}

set_progress_bar <- function(total) {
  progress_bar$new(
    total = total, clear = FALSE, width = 70,
    format = ":current/:total [:bar] - :elapsed - loss: :loss - acc: :acc"
  )
}
```


```{r}
epochs <- 10
losses <- c()
accs <- c()


coro::loop(for(epoch in seq_len(epochs)) {
  
  pb <- set_progress_bar(length(ds_train))
  pb$message(glue("Epoch {epoch}/{epochs}"))
  for(batch in enumerate(ds_train)) {
    optimizer$zero_grad()
    predictions <- model(batch[[1]]$to(device = device))
    targets <- batch[[2]]$to(device = device)
    loss <- loss_criterion(predictions, targets)
    loss$backward()
    optimizer$step()
    
    # eval reports
    prediction_r <- pred_to_r(predictions$argmax(dim = 2))
    targets_r <- pred_to_r(targets)
    acc <- metrics$acc(targets_r, prediction_r)
    accs <- c(accs, acc)
    loss_r <- as.numeric(loss$item())
    losses <- c(losses, loss_r)
    
    pb$tick(tokens = list(loss = round(mean(losses), 4), acc = round(mean(accs), 4)))
  }
  
  # test
  predictions_r <- c()
  targets_r <- c()
  for(batch_test in enumerate(ds_test)) {
    predictions <- model(batch_test[[1]]$to(device = device))
    targets <- batch_test[[2]]$to(device = device)
    predictions_r <- c(predictions_r, pred_to_r(predictions$argmax(dim = 2)))
    targets_r <- c(targets_r, pred_to_r(targets))
  }
  predictions_r <- predictions_r[predictions_r >= 0]
  targets_r <- targets_r[targets_r >= 0]
  val_acc <- metrics$acc(factor(targets_r, levels = 1:30), factor(predictions_r, levels = 1:30))
  cat(glue("val_acc: {val_acc}\n"))
})
```

## Making predictions

We alaready have all predictions calculated for `test_subset`, let's recreate the alluvial plot from original article.

```{r}
library(dplyr)
library(alluvial)
df_validation <- data.frame(
  pred_class = df$classes[predictions_r],
  class = df$classes[targets_r]
)
x <-  df_validation %>%
  mutate(correct = pred_class == class) %>%
  count(pred_class, class, correct)

alluvial(
  x %>% select(class, pred_class),
  freq = x$n,
  col = ifelse(x$correct, "lightblue", "red"),
  border = ifelse(x$correct, "lightblue", "red"),
  alpha = 0.6,
  hide = x$n < 20
)
```

Model accuracy is `r scales::percent(val_acc, accuracy = 0.01)`, similar to tensorflow version from the original post. Likewise, all conclusions from original post also holds.

